import streamlit as st
import cv2
import numpy as np
import av
# WebRtcMode ã‚’è¿½åŠ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration, WebRtcMode

# --- UIè¨­å®š ---
st.set_page_config(page_title="ç©¿åˆºã‚¬ã‚¤ãƒ‰ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿", layout="wide")
st.title("ğŸ’‰ ç©¿åˆºã‚¬ã‚¤ãƒ‰ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ (ã‚¯ãƒ©ã‚¦ãƒ‰å¯¾å¿œç‰ˆ)")
st.caption("Ver 1.1 - Fixed Mode & Connection")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š ---

# 1. ã‚«ãƒ¡ãƒ©åˆ‡ã‚Šæ›¿ãˆ
st.sidebar.header("ğŸ“· ã‚«ãƒ¡ãƒ©è¨­å®š")
camera_mode = st.sidebar.radio(
    "ã‚«ãƒ¡ãƒ©ã®å‘ã",
    ("ã‚¤ãƒ³ã‚«ãƒ¡ãƒ© (è‡ªåˆ†å´)", "ã‚¢ã‚¦ãƒˆã‚«ãƒ¡ãƒ© (å¤–å´)"),
    index=1
)

if camera_mode == "ã‚¤ãƒ³ã‚«ãƒ¡ãƒ© (è‡ªåˆ†å´)":
    # PCã‚„ã‚¤ãƒ³ã‚«ãƒ¡ãƒ©ç”¨
    video_constraints = {"facingMode": "user", "width": {"ideal": 640}, "height": {"ideal": 480}}
else:
    # ã‚¹ãƒãƒ›ã®ã‚¢ã‚¦ãƒˆã‚«ãƒ¡ãƒ©ç”¨ï¼ˆenvironmentï¼‰
    video_constraints = {"facingMode": "environment", "width": {"ideal": 640}, "height": {"ideal": 480}}

# 2. ã‚¬ã‚¤ãƒ‰æ©Ÿèƒ½
st.sidebar.markdown("---")
st.sidebar.header("ğŸ“ ã‚¬ã‚¤ãƒ‰è¨­å®š")
show_guide = st.sidebar.checkbox("ç–‘ä¼¼é‡ï¼ˆã‚¬ã‚¤ãƒ‰ç·šï¼‰ã‚’è¡¨ç¤º", value=True)
guide_length_mm = st.sidebar.slider("ç–‘ä¼¼é‡ã®é•·ã• (mm)", 1.0, 5.0, 3.0, step=0.5)

# 3. èª¿æ•´ç”¨
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ‘€ èª¿æ•´")
show_edge = st.sidebar.checkbox("ã‚¨ãƒƒã‚¸ã®ã¿è¡¨ç¤º (èªè­˜ç¢ºèª)", value=False)


# --- Logic: æ˜ åƒå‡¦ç†ã‚¯ãƒ©ã‚¹ ---
class NeedleGuideSimulator(VideoProcessorBase):
    def __init__(self):
        self.show_guide = True
        self.guide_len_mm = 3.0
        self.debug_mode = False
        self.PX_PER_MM = 20.0  # â€»ä»®å®šå€¤: å®Ÿéš›ã®è·é›¢æ ¡æ­£ã¯åˆ¥é€”å¿…è¦

    def update_settings(self, guide_on, guide_len_mm, debug):
        self.show_guide = guide_on
        self.guide_len_mm = guide_len_mm
        self.debug_mode = debug

    def recv(self, frame):
        try:
            # WebRTCã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’numpyé…åˆ—(BGR)ã«å¤‰æ›
            img = frame.to_ndarray(format="bgr24")
            
            # --- ç”»åƒå‡¦ç†ãƒ—ãƒ­ã‚»ã‚¹ ---
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            blurred = cv2.GaussianBlur(gray, (5, 5), 0)
            edges = cv2.Canny(blurred, 50, 150)
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ãªã‚‰ã‚¨ãƒƒã‚¸ç”»åƒã ã‘ã‚’è¿”ã™
            if self.debug_mode:
                return av.VideoFrame.from_ndarray(cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR), format="bgr24")

            # ç›´ç·šæ¤œå‡º (Houghå¤‰æ›)
            lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=50, minLineLength=60, maxLineGap=20)

            best_line = None
            max_len = 0
            current_angle = 0.0
            
            if lines is not None:
                for line in lines:
                    lx1, ly1, lx2, ly2 = line[0]
                    
                    # è§’åº¦è¨ˆç®— (0é™¤ç®—å›é¿)
                    if lx2 - lx1 == 0: 
                        la = 90.0
                    else: 
                        la = np.degrees(np.arctan2(abs(ly2 - ly1), abs(lx2 - lx1)))
                    
                    # ç©¿åˆºè§’åº¦ã¨ã—ã¦ã‚ã‚Šå¾—ã‚‹ç¯„å›²(10åº¦ã€œ85åº¦)ã®ç·šã ã‘æ¡ç”¨
                    if 10 < la < 85:
                        length = np.sqrt((lx2 - lx1)**2 + (ly2 - ly1)**2)
                        # æœ€ã‚‚é•·ã„ç·šã‚’ã€Œé‡ã€ã¨ã¿ãªã™
                        if length > max_len:
                            max_len = length
                            best_line = line
                            current_angle = la
                
                # ãƒ™ã‚¹ãƒˆãªç·šãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®æç”»å‡¦ç†
                if best_line is not None:
                    bx1, by1, bx2, by2 = best_line[0]
                    
                    # é‡å…ˆåˆ¤å®š: ç”»é¢ã®ä¸‹å´(yãŒå¤§ãã„æ–¹)ã‚’æ ¹æœ¬ã€ä¸Šå´ã‚’é‡å…ˆã¨ä»®å®šã™ã‚‹ç°¡æ˜“ãƒ­ã‚¸ãƒƒã‚¯
                    if by1 < by2: 
                        tip = (bx1, by1); tail = (bx2, by2)
                    else: 
                        tip = (bx2, by2); tail = (bx1, by1)
                    
                    # è§’åº¦ã«ã‚ˆã‚‹è‰²åˆ†ã‘ (ä¾‹: 20-40åº¦ãŒæ¨å¥¨ç¯„å›²ãªã‚‰ã‚ªãƒ¬ãƒ³ã‚¸ã€ãã‚Œä»¥å¤–ã¯é»„è‰²)
                    status_color = (0, 255, 255) # Yellow
                    if 20 <= current_angle <= 40:
                        status_color = (0, 165, 255) # Orange (BGR)
                    
                    # å®Ÿç·šã®æç”»
                    cv2.line(img, tail, tip, status_color, 6)
                    
                    # ã‚¬ã‚¤ãƒ‰ç·š(å»¶é•·ç·š)ã®æç”»
                    if self.show_guide:
                        vec_x = tip[0] - tail[0]
                        vec_y = tip[1] - tail[1]
                        vec_len = np.sqrt(vec_x**2 + vec_y**2)
                        
                        if vec_len > 0:
                            unit_x = vec_x / vec_len
                            unit_y = vec_y / vec_len
                            
                            pixel_length = self.guide_len_mm * self.PX_PER_MM
                            
                            guide_end_x = int(tip[0] + unit_x * pixel_length)
                            guide_end_y = int(tip[1] + unit_y * pixel_length)
                            
                            # ã‚¬ã‚¤ãƒ‰ç·šæç”»
                            cv2.line(img, tip, (guide_end_x, guide_end_y), (255, 255, 0), 3)
                            # å…ˆç«¯ã®ç‚¹
                            cv2.circle(img, (guide_end_x, guide_end_y), 5, (255, 255, 0), -1)

                    # è§’åº¦è¡¨ç¤º
                    msg = f"Angle: {current_angle:.1f}"
                    cv2.putText(img, msg, (tail[0], tail[1]-20), cv2.FONT_HERSHEY_SIMPLEX, 1, status_color, 2)

            return av.VideoFrame.from_ndarray(img, format="bgr24")

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¦ã‚‚ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’æ­¢ã‚ãªã„ï¼ˆã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã¯å‡ºã™ï¼‰
            print(f"Error processing frame: {e}")
            return frame

# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨ ---

# ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒç”¨ã®å¼·åŠ›ãªSTUNè¨­å®š
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# Streamlit UIã¸ã®é…ç½®
ctx = webrtc_streamer(
    key="needle-cloud-mode",
    mode=WebRtcMode.SENDRECV, # <--- ã€é‡è¦ä¿®æ­£ã€‘æ–‡å­—åˆ—ã§ã¯ãªãEnumã‚’ä½¿ç”¨
    rtc_configuration=RTC_CONFIGURATION,
    video_processor_factory=NeedleGuideSimulator,
    media_stream_constraints={"video": video_constraints, "audio": False},
    async_processing=True,
)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‹•çš„åæ˜ 
if ctx.video_processor:
    ctx.video_processor.update_settings(show_guide, guide_length_mm, show_edge)
